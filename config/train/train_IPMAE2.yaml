EXNAME: ipmae8lunirate10clipconres

IPMAE:
  arch: trans_enc
  device: cuda
  d_input: 263
  max_motion_len: 196
  TRMDECODER:
    d_model: 512
    nhead: 8
    d_ffn: 2048
    nlayer: 8
    dropout: 0.1
    batch_first: True
  MASK:
    method: uniform
    ratio: 0.90

TEXTENCODERLIST: ['RobertaModel', 'ElectraModel', 'T5Model', 'CLIPModel']
TEXTENCODERMODELLIST: ['roberta-base', 'google/electra-base-discriminator','t5-base', 'clip-vit-large-patch14']

SDMTEXTENCODER:
  # d_model: 768
  # NAME: RobertaModel
  # TOKENIZER: RobertaTokenizer
  # ENCODERPATH: 'pretraining/roberta-base'
  # TOKENIZERNAME: RobertaTokenizer 
  d_model: 768
  NAME: CLIPModel
  TOKENIZER: CLIPProcessor
  ENCODERPATH: ./pretraining/clip-vit-large-patch14
  TOKENIZERNAME: CLIPProcessor
  device: cuda

TRAIN:
  EPOCHS: 500
  lr: 0.0001
  device: cuda
DATALOADER:
  NAME: humanml
  batch_size: 32
  NFEATS: 263
  UNIT_LEN: 4
  mean_path: 

device: cuda



model:
  t2m_textencoder:
    dim_word: 300
    dim_pos_ohot: 15
    dim_text_hidden: 512
    dim_coemb_hidden: 512
    target: mld.models.architectures.t2m_textenc.TextEncoderBiGRUCo
    params:
      word_size: 300
      pos_size: 15
      hidden_size: 512
      output_size: 512
  t2m_motionencoder:
    dim_move_hidden: 512
    dim_move_latent: 512
    dim_motion_hidden: 1024
    dim_motion_latent: 512
    target: mld.models.architectures.t2m_motionenc.MotionEncoder
    params:
      input_size: ${model.t2m_moveencoder.output_size}
      hidden_size: 1024
      output_size: 512  

  t2m_moveencoder:
    target: mld.models.architectures.t2m_textenc.MovementConvEncoder
    params:
      hidden_size: 512
      output_size: 512
  t2m_path: ./pretraining/t2m

METRIC:
  TYPE: ['TM2TMetrics'] # ['TemosMetric', 'TM2TMetrics']
  FORCE_IN_METER: true
  DIST_SYNC_ON_STEP: true